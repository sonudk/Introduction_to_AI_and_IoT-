{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BHAKTI TIME - 190608_kerasbasics_gaussian-assignment_f.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonudk/Introduction_to_AI_and_IoT-/blob/master/BHAKTI_TIME_190608_kerasbasics_gaussian_assignment_f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QrZqmW-YtUKw"
      },
      "source": [
        "# Keras Basics\n",
        "We will learn about\n",
        "* Dense layers\n",
        "* Categorical cross-entropy\n",
        "\n",
        "A toy example to show how to train a classifier with Keras and use it. The data comes from three gaussian distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eWyR310TtUKy",
        "colab": {}
      },
      "source": [
        "## DATA GENERATION\n",
        "import numpy as np\n",
        "\n",
        "def generateX(cls):\n",
        "    '''\n",
        "    Inputs:\n",
        "        cls: class {0, 1, 2}\n",
        "    Outputs:`w\n",
        "        x: a sample from cls; a np array of shape (2,)\n",
        "    '''\n",
        "    assert cls in [0,1,2]\n",
        "    if cls==0:\n",
        "        x = np.random.normal(np.array([0,0]),100)\n",
        "    elif cls==1:\n",
        "        x = np.random.normal(np.array([200,200]),100)\n",
        "    elif cls==2:\n",
        "        x = np.random.normal(np.array([-200,200]),100)\n",
        "    return x\n",
        "Nx = 2 # shape of a sample is (2,)\n",
        "Ny = 3 # 3 classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "24sOXTIttUK2"
      },
      "source": [
        "Could you write a function to generate N samples from class 0 and N samples from class 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "9W3ccvYYtUK4",
        "nbgrader": {
          "checksum": "71c5837a9d68fac4398e11bcb87c3bd2",
          "grade": false,
          "grade_id": "cell-6ee804e3860f2ff6",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def generateXY(N):\n",
        "    '''\n",
        "    Inputs:\n",
        "        N: no. of samples of each class\n",
        "    Outputs:\n",
        "        X: np array of samples; shape = (3*N, 2)\n",
        "        Y: np array of samples; shape = (3*N, 1)\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    classes = [0,1,2]\n",
        "    X = np.empty([3*N,2])\n",
        "    Y = np.empty([3*N,1])\n",
        "    for c in classes:\n",
        "      for i in range(N):\n",
        "        X[i + c*N] = generateX(c)\n",
        "        Y[i + c*N] = c\n",
        "    '''print(X)\n",
        "    print(Y)'''\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "editable": false,
        "id": "sUqeO_NWtUK7",
        "nbgrader": {
          "checksum": "c0183471e369c049b734441886caaff4",
          "grade": true,
          "grade_id": "cell-ad908829419fd089",
          "locked": true,
          "points": 1,
          "schema_version": 1,
          "solution": false
        },
        "outputId": "57a47d76-5e11-418e-ce57-05076d66f935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_generateXY():\n",
        "    X_train, Y_train = generateXY(50)\n",
        "    assert X_train.shape==(150,2)\n",
        "    assert Y_train.shape==(150,1)\n",
        "    print('Test passed', '\\U0001F44D')\n",
        "test_generateXY()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed ðŸ‘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJ3TNcj-tULI"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "Now our Y is in the form [0], [1] and [2]. We want to convert them to [1,0,0], [0,1,0] and [0,0,1], respectively. \n",
        "Could you write a code to convert Y (with one column) into one-hot encoded Y (with 3 columns)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "_n4fYMC0tULJ",
        "nbgrader": {
          "checksum": "2920fc139021b2f772982b2e16731703",
          "grade": false,
          "grade_id": "cell-db496b9b86c28424",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def oneHot(y, Ny):\n",
        "    '''\n",
        "    Input:\n",
        "        y: an int in {0, 1, 2}\n",
        "        Ny: Number of classes, e.g., 3 here.\n",
        "    Output:\n",
        "        Y: a vector of Ny (=3) tuples\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    y = int(y)\n",
        "    y_onehot = np.eye(Ny)\n",
        "    Y = y_onehot[y]\n",
        "    return Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "editable": false,
        "id": "fq8OZ0cxtULM",
        "nbgrader": {
          "checksum": "8612cec704a627b66ff552899569f828",
          "grade": true,
          "grade_id": "cell-24fb717c7ea66826",
          "locked": true,
          "points": 2,
          "schema_version": 1,
          "solution": false
        },
        "outputId": "4794e4b0-87e4-4e8f-b99d-55371a18877c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_oneHot():\n",
        "    assert np.all(oneHot(0,3)==np.array([1,0,0]))\n",
        "    assert np.all(oneHot(1,3)==np.array([0,1,0]))\n",
        "    assert np.all(oneHot(2,3)==np.array([0,0,1]))\n",
        "    print('Test passed', '\\U0001F44D')\n",
        "test_oneHot()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed ðŸ‘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EzQywDiStULR"
      },
      "source": [
        "### Input Normalization\n",
        "X can lie in any unbounded range. We need to curtail to a narrow range close to zero. This helps in enhancing the stability of training and hyper-parameter tuning.\n",
        "This is normally achieved by scaling the X to have zero mean and unit standard deviation (std).\n",
        "\n",
        "$X \\leftarrow \\frac{X-mean(X)}{std(X)}$, where this is element wise division\n",
        "\n",
        "Could you use training samples to find mean and std, and normalize your X_train with that?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "_v_HEe10tULS",
        "nbgrader": {
          "checksum": "4d88f9abc4004f238b182e54336e76e2",
          "grade": false,
          "grade_id": "cell-8564364c76ddcdc7",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def findMeanStddev(X):\n",
        "    '''\n",
        "    Input: \n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "    Output:\n",
        "        mean: mean of samples in X; shape is (dimension of each sample,)\n",
        "        stddev: element-wise std dev of sample in X; shape is (dimension of each sample,)\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    mean = np.sum(X,axis = 0)/X.shape[0]\n",
        "    X1 = X - mean\n",
        "    stddev = np.sqrt(np.sum(X1*X1,axis = 0)/X.shape[0])\n",
        "    #print(mean,stddev)\n",
        "    return mean, stddev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "editable": false,
        "id": "qYUioNiqxRyi",
        "nbgrader": {
          "checksum": "5d5ccf5b778b190a8607fe045aebce74",
          "grade": true,
          "grade_id": "cell-c060c271af9064e7",
          "locked": true,
          "points": 2,
          "schema_version": 1,
          "solution": false
        },
        "outputId": "d5798f4a-768f-467e-c7ef-33531b42839b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_findMeanStddev():\n",
        "    X = np.array([[3,2,6],[7,4,2],[3,5,1]])\n",
        "    mean, stddev = findMeanStddev(X)\n",
        "    assert np.isclose(mean, np.array([4.33, 3.66, 3.]), atol=0.1).all()\n",
        "    assert np.isclose(stddev, np.array([1.88, 1.24, 2.16]), atol=0.1).all()\n",
        "    print('Test passed', '\\U0001F44D')\n",
        "test_findMeanStddev()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed ðŸ‘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "gG4sm2WfwRqu",
        "nbgrader": {
          "checksum": "6c5fc8807584cce426d6dd8fd10dfc7c",
          "grade": false,
          "grade_id": "cell-80ad17d9f5962f88",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def normalizeX(X, mean, stddev):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X) \n",
        "    Output:\n",
        "        Xn: X modified to have 0 mean and 1 std dev\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    Xn = (X - mean)/(stddev + 1e-8)\n",
        "    return Xn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "editable": false,
        "id": "t4JdFDb7tULZ",
        "nbgrader": {
          "checksum": "cb4af2655a94d3e991efe9c64ba57a8c",
          "grade": true,
          "grade_id": "cell-0880b9b53201680b",
          "locked": true,
          "points": 2,
          "schema_version": 1,
          "solution": false
        },
        "outputId": "55a0056b-e5be-4a21-a34f-12340968b7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_normalizeX():\n",
        "    X = np.ones((3,3))\n",
        "    m,s = findMeanStddev(X)\n",
        "    assert np.all(m==np.ones(3))\n",
        "    assert np.all(s==np.zeros(3))\n",
        "    assert np.all(normalizeX(X,m,s)==0*X)\n",
        "    # test on random X\n",
        "    X = np.random.random((5,3))\n",
        "    m,s = findMeanStddev(X)\n",
        "    Xn = normalizeX(X,m,s)\n",
        "    mn, sn = findMeanStddev(Xn)\n",
        "    assert np.allclose(mn, np.zeros(3))\n",
        "    assert np.allclose(sn, np.ones(3))\n",
        "    print('Test passed', '\\U0001F44D')\n",
        "test_normalizeX()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed ðŸ‘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1zpEX8EEtULe"
      },
      "source": [
        "### Plotting\n",
        "Could you plot all the samples in X_train with different colors for different classes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JfkkWGWZtULf",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "def plotXY(X, Y):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "        Y: a matrix of size (no. of samples, no. of classes) - these are one-hot vectors\n",
        "    Action:\n",
        "        Plots the samples in X, their color depends on Y\n",
        "    '''\n",
        "    Ny = Y.shape[1]\n",
        "    for cls in range(Ny):\n",
        "        idx = np.where(Y[:,cls]==1)[0]\n",
        "        plt.plot(X[idx,0], X[idx,1], colors[cls]+'.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kB3jK6IrtULk"
      },
      "source": [
        "## Creating the Network\n",
        "We now create the network with dense layers: \n",
        "$y = f(Wx)$\n",
        "\n",
        "ReLU activation: \n",
        "$f(h) = h, h>0; 0, h\\le 0$\n",
        "\n",
        "Softmax activation: \n",
        "$f(h_i) = \\frac{\\exp(h_i)}{\\sum_j \\exp(h_j)}$\n",
        "\n",
        "Categorical cross-entropy loss:\n",
        "$\\mathcal{L} = -\\sum_t y^d_t \\log y_t$\n",
        "\n",
        "Stochastic Gradient Descent:\n",
        "$w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "g60Qem92tULl",
        "nbgrader": {
          "checksum": "1f332e46663382a71f1dfa333725d807",
          "grade": false,
          "grade_id": "cell-e18133df577f7820",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense ,Activation\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "\n",
        "def makeNN(Nx, Nh, Ny):\n",
        "    '''\n",
        "    Input:\n",
        "        Nx: int; no. of input nodes; shape of each sample; i.e., X.shape[1:] \n",
        "        Nh: int; no. of hidden neurons\n",
        "        Ny: int; no. of output nodes; shape of output; i.e., Y.shape[1]\n",
        "    Output:\n",
        "        model: keras NN model with Input layer, Dense layer with Nh neurons, \n",
        "                and Dense output layer with softmax non-linearity, loss function\n",
        "                categorical-crossentropy, optimizer SGD.\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    x = Input(shape=(Nx,))\n",
        "    y = Dense(Nh, activation='relu')(x)\n",
        "    y = Dense(Ny, activation='softmax')(y)\n",
        "    model = Model(inputs=x, outputs=y)\n",
        "    model.compile(optimizer=optimizers.sgd(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    #model.summary()\n",
        "    '''  model= Model()@########### model.add is not availabel in Model algo. of keras it is available in Sequential of keras\n",
        "    model.add(Dense(Nh,input_dim = Nx,activation = 'sigmoid'))\n",
        "    model.add(Desne(Ny,activatoin = 'softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy',optimizer = 'sgd',metrics = ['accuracy'])\n",
        "    model.summary()'''\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q-Gf4DSltULt"
      },
      "source": [
        "### Plotting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0qkPv1xptULu",
        "colab": {}
      },
      "source": [
        "def plotModel(model):\n",
        "    from keras.utils import plot_model\n",
        "    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "    from IPython.display import Image\n",
        "    Image(retina=True, filename='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XO26y8VZtULz"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "Yfvm6pH5tUL0",
        "nbgrader": {
          "checksum": "78b5d80f2ec4bcbc57d80a6bc475f285",
          "grade": false,
          "grade_id": "cell-8a4f621147d44a84",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def trainNN(model, X_train, Y_train, Nepochs):\n",
        "    '''\n",
        "    Action:\n",
        "        Train model with model.fit\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    model.fit(X_train,Y_train,epochs=Nepochs,verbose = 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upKWTY-dVRIv",
        "colab_type": "code",
        "outputId": "d7ff847c-a548-4d8a-9df6-0905fd403080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''y = np.array([[1,2,2,1,0]])\n",
        "Y_onehot = np.empty([y.shape[0],3])\n",
        "for i in range(y.shape[0]):\n",
        "  Y_onehot[i] = oneHot(y[i],3)\n",
        "print(Y_onehot)'''\n",
        "'''Y = np.eye(4)\n",
        "Y[3] = [2,3,4,2]\n",
        "print(Y[3])'''"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Y = np.eye(4)\\nY[3] = [2,3,4,2]\\nprint(Y[3])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "QZRghwIG2I5c",
        "nbgrader": {
          "checksum": "5d34ead5470a39daddbd54aa7f19ef1f",
          "grade": false,
          "grade_id": "cell-c45fc6de4c3fc4c9",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def trainModel(N, Nh, Nepochs):\n",
        "    '''\n",
        "    generateXY, normalizeX, oneHot, makeNN, trainNN\n",
        "    Input:\n",
        "        N: int; no. of training samples per class\n",
        "        Nh: int; no. of neurons in hidden layer\n",
        "    Output:\n",
        "        model: keras NN model trained with the training data\n",
        "        mean_train, stddev_train: mean and stddev of training data - you will \n",
        "                            need this for normalizing your test data\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    X,Y = generateXY(N)\n",
        "    Ny = 3\n",
        "    Y_onehot = np.empty((Y.shape[0],Ny))\n",
        "    for i in range(Y.shape[0]):\n",
        "      Y_onehot[i] = oneHot(Y[i],Ny)\n",
        "    mean_train,stddev_train = findMeanStddev(X)\n",
        "    X = normalizeX(X,mean_train,stddev_train)\n",
        "    model = makeNN(X.shape[1],Nh,Y_onehot.shape[1])\n",
        "    trainNN(model, X , Y_onehot, Nepochs)\n",
        "    return model, mean_train, stddev_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y7a1LEcgtUL4"
      },
      "source": [
        "### Evaluation\n",
        "Could you:\n",
        "- Generate 20 samples from each class\n",
        "- Normalize them with mean_train and stddev_train\n",
        "- Get Y_test as one hot encoded labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "dkj4meV_tUL9",
        "nbgrader": {
          "checksum": "1c367e3d2d36a80eb1fffbb27eb32da8",
          "grade": false,
          "grade_id": "cell-02b3b15dc3f435fe",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def testModel(model, Ntest, mean_train, stddev_train):\n",
        "    '''\n",
        "    generateXY for test, normalize, onehot, evaluate the model\n",
        "    Inputs:\n",
        "        model: trained Keras NN model\n",
        "        Ntest: int; number of test samples per class\n",
        "    Output:\n",
        "        accuracy: float; accuracy on the test data\n",
        "        CM: confusion matrix on the test data\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    Ny = 3\n",
        "    X_test,Y_test = generateXY(Ntest)\n",
        "    X_test = normalizeX(X_test,mean_train,stddev_train)\n",
        "    Y_test_onehot = np.zeros((Y_test.shape[0],Ny))\n",
        "    for i in range(Y_test.shape[0]):\n",
        "      Y_test_onehot[i] = oneHot(Y_test[i],Ny)\n",
        "    loss , accuracy = model.evaluate(X_test,Y_test_onehot,verbose = 0)\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    y_pred = model.predict(X_test)\n",
        "    CM = confusion_matrix(Y_test_onehot.argmax(axis = 1),y_pred.argmax(axis = 1))\n",
        "    return accuracy, CM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N9AKg5YBtUMB",
        "outputId": "dc22cd9a-9269-4acb-aea4-0fe15cdacdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model, mean_train, stddev_train = trainModel(50, 10, 100)\n",
        "accuracy, CM = testModel(model, 10, mean_train, stddev_train)\n",
        "print('Accuracy:',accuracy)\n",
        "print('CM:',CM)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6000000238418579\n",
            "CM: [[10  0  0]\n",
            " [ 0  1  9]\n",
            " [ 3  0  7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1WdkjiA2AZYj"
      },
      "source": [
        "# ADVANCED QUESTIONS\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFBjZFLnA100"
      },
      "source": [
        "### Effect of changing Nh\n",
        "### Effect of changing Nepochs\n",
        "### Effect of changing N, no. of training samples\n",
        "\n",
        "Can you observe overfitting? \n",
        "\n",
        "Can you do hyperparameter tuning here? \n",
        "\n",
        "To normalize test data, why do we use the mean and stddev of training data?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mAjmJ6IJn5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1992e26e-ade4-4d5c-920e-1f5538177865"
      },
      "source": [
        "#effect of Nh\n",
        "Nh_all = [1,3,5,6,7,8,9,10,15,17]\n",
        "Nepochs = 20\n",
        "N = 50\n",
        "Ntest =20\n",
        "acc_list = []\n",
        "for i in Nh_all:\n",
        "  model, mean_train, stddev_train = trainModel(N, i , Nepochs)\n",
        "  accuracy , CM = testModel(model, Ntest , mean_train, stddev_train)\n",
        "  acc_list.append(accuracy)\n",
        "print(np.array(acc_list))  "
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.30000001 0.45       0.83333334 0.11666667 0.28333333 0.46666667\n",
            " 0.15       0.55       0.21666667 0.59999999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXJ9hs_0oekX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}