{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation_Metrics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonudk/Introduction_to_AI_and_IoT-/blob/master/Evaluation_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O22xUTBA4K5y"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lHD-hDNs9hXH",
        "outputId": "eef18482-ca9d-43a3-e65a-21f70c9a0288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "expected = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
        "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
        "results = confusion_matrix(expected, predicted)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4 2]\n",
            " [1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5jFmBgO8zYVH"
      },
      "source": [
        "<img src=https://cdn-images-1.medium.com/max/800/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg\n",
        "alt=\"Confusion matrix!\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sXtpu3cR9n3u"
      },
      "source": [
        "### Accuracy\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP}{\\text{Total}} = \\frac{TP+TN}{TP+FP+TN+FN}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-UKeqTWZ91OB",
        "outputId": "86cbf4c7-c0b7-4672-ef11-12a7020cb2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy = (3+4)/(4+2+1+3)\n",
        "accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHqS1xWmh2Xw"
      },
      "source": [
        "But there are problems associated with accuracy metric. Consider this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W_7qJI2HiVjf",
        "outputId": "b453298f-636f-4faa-f68e-214eda469eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "cm = np.array([[990, 0], [10, 0]])\n",
        "print(cm)\n",
        "\n",
        "accuracy = (990+0)/(1000)\n",
        "print('accuracy is', accuracy )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[990   0]\n",
            " [ 10   0]]\n",
            "accuracy is 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8zlXQO1BjXTk"
      },
      "source": [
        "Here, accuracy is 99% but most of the samples are true negatives. <br>\n",
        "If this is a case of a sick patient with a contagious disease, then this classifier is not good as it classifies true positives as false negatives and the cost of such a mis-classification is very high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zN2YXZNU_vqb"
      },
      "source": [
        "### Precision\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{\\text{Predicted Positives}} = \\frac{TP}{TP+FP}\n",
        "$$\n",
        "\n",
        "- Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive. \n",
        "- Precision is a good measure to determine, when the costs of False Positive is high. <br>\n",
        "  Eg., For Eg., in Email spam detection, if a non-spam email (True Negative) has been classified as spam email (False positive), then the recipient\n",
        "   may lose important information. <br>\n",
        "   <img src=https://cdn-images-1.medium.com/max/800/1*PULzWEven_XAZjiMNizDCg.png\n",
        "alt=\"Confusion matrix!\" />\n",
        "   \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ngijq16Z_8PF",
        "outputId": "bb3a5b22-4575-4f98-dae3-81306084743c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precision = 4/(4+1)\n",
        "precision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9NtjNpwgAd4-"
      },
      "source": [
        "### Recall\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{\\text{Actual Positives}} = \\frac{TP}{TP+FN}\n",
        "$$\n",
        " <img src=https://cdn-images-1.medium.com/max/800/1*BBhWQC-m0CLN4sVJ0h5fJQ.jpeg\n",
        "alt=\"Confusion matrix!\" />\n",
        "- Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative <br>\n",
        "- For eg, in sickness detection, if a sick patient (True positive) is predicted as not - sick (False Negative), the associated cost may be very high if the disease is contagious. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F845jzgGAjeA",
        "outputId": "13f608c3-fb93-4227-9caf-26f1f53f567e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "recall = 3/(4+2)\n",
        "recall"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7kp5rVzmCCmg"
      },
      "source": [
        "### F - score\n",
        "$F_1$ score is the harmonic mean of precision and recall\n",
        "$$\n",
        "F_1 = \\left(\\frac{recall^{-1} + precision^{-1}}{2}\\right)^{-1} = 2 \\cdot \\frac{precision.recall}{precision+recall}\n",
        "$$\n",
        "- F1 Score is needed when you want to seek a balance between Precision and Recall.\n",
        "- We saw that accuracy is not useful when classes are unevenly balanced - No. of True negative samples is very high. <br>\n",
        "  In such a case, F1 score is a better measure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bjCxCV93EGya",
        "outputId": "537aa3a1-02dc-4f64-9301-25d677a5ea19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f_1_score = 2*(0.6)*(0.5)/(0.6+0.5)\n",
        "f_1_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5454545454545454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZVLqqMLPFl4u"
      },
      "source": [
        "### False Acceptance Rate\n",
        "$$\n",
        "FAR = \\frac{FP}{TP + FP} \n",
        "$$\n",
        "\n",
        "- False acceptance rate is the measure of the likelihood that a system will classify a true negative sample as a false positive.\n",
        "- A high $FAR$ is extremely harmful when the cost of a false positive is very high.\n",
        "- For eg., in biometric systems that grant authorization to users, giving access to an unauthorized person (true negative) can be devastating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGHRQ_LGpt1r",
        "colab_type": "text"
      },
      "source": [
        "### False Rejection Rate\n",
        "$$\n",
        "FRR = \\frac{FN}{FN + TN}\n",
        "$$\n",
        "- False rejection rate is the measure of the likelihood that a system will classify a true positive sample as a false negative.\n",
        "- A high FRR is harmful when the cost of a false negative is very high. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkpvX_Kmpt1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}